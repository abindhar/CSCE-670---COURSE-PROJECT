{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make DataSets\n",
    "\n",
    "1. All features + Reduced Embeddings 6 [done]\n",
    "2. All features + Reduced Embeddings 8 [done]\n",
    "3. All features + AllEmbeddings [done]\n",
    "4. GE(10) + W2V(6) [done]\n",
    "5. GE(10) + W2V(8) [done]\n",
    "6. GE(10) + AllEmbeddings [done]\n",
    "7. GE(100) + W2V(6) [done]\n",
    "8. GE(100) + W2V(8) [done]\n",
    "9. GE(100) + AllEmbeddings [done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from collections import OrderedDict\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path=\"D:\\\\Courses\\\\InfoStorage\\\\Project\\\\Data\\\\FE\"\n",
    "item_stats= pd.read_pickle(pickle_path+\"\\\\FeaturePickle\\\\product_stats.pkl\")\n",
    "queries_input= pd.read_pickle(pickle_path+\"\\\\FeaturePickle\\\\queries_input.pkl\")\n",
    "query_item = pd.read_pickle(pickle_path+\"\\\\FeaturePickle\\\\query_item.pkl\")\n",
    "item_graphEmbedding10 = pd.read_pickle(pickle_path+\"\\\\item_GraphEmbeddings10.pkl\")\n",
    "item_graphEmbedding100 = pd.read_pickle(pickle_path+\"\\\\item_GraphEmbeddings100.pkl\")\n",
    "vector_gensimProduct6 = pd.read_pickle(pickle_path+\"\\\\Vector_gensim_product_6.pkl\")\n",
    "vector_gensimProduct8 = pd.read_pickle(pickle_path+\"\\\\Vector_gensim_product_8.pkl\")\n",
    "vector_gensimQuery6 = pd.read_pickle(pickle_path+\"\\\\Vector_gensim_query_6.pkl\")\n",
    "vector_gensimQuery8 = pd.read_pickle(pickle_path+\"\\\\Vector_gensim_query_8.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stats.fillna(value=0, inplace=True)\n",
    "X = pd.merge(query_item,item_stats,on='itemId',how='left')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphEmbedding10 + W2V(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_graphEmbedding100.head()\n",
    "c=[]\n",
    "for i in range(0,16):\n",
    "    c.append(\"graphfeature\"+str(i))\n",
    "c.append(\"itemId\")\n",
    "item_graphEmbedding100.columns=c\n",
    "item_graphEmbedding100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save=\"D:\\\\Courses\\\\InfoStorage\\\\Project\\\\Data\\\\Numpy_Objects\\\\GraphFeatures100WithWordEmbeddingsALL\\\\\"\n",
    "basicfeatures_cosine= pd.read_pickle(path_to_save+\"basicfeatures_cosine.pkl\")\n",
    "basicfeatures_cosine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop([ \\\n",
    "           'item_show_count','Item_clicks','Item_view_count','Item_purchase_count','userShow','userView',\\\n",
    "            'userPurchase','CTR','View Rate','Click Value Rate'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicfeatures_cosine = basicfeatures_cosine.drop([ \\\n",
    "           'item_show_count','Item_clicks','Item_view_count','Item_purchase_count','userShow','userView',\\\n",
    "            'userPurchase','CTR','View Rate','Click Value Rate','Relevance','is.test'],axis=1)\n",
    "basicfeatures_cosine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X,basicfeatures_cosine,on=['queryId','itemId'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X['vector_gensim_query'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X,item_graphEmbedding100,on='itemId',how='left')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X = pd.merge(X,vector_gensimProduct6,on='itemId',how='left')\n",
    "#X = pd.merge(X,vector_gensimQuery6,on='queryId',how='left')\n",
    "#X = pd.merge(X,vector_gensimProduct8,on='itemId',how='left')\n",
    "#X = pd.merge(X,vector_gensimQuery8,on='queryId',how='left')\n",
    "\n",
    "Train = X[X['is.test']==False]\n",
    "Test = X[X['is.test']== True]\n",
    "Train= Train.drop(['is.test'], axis=1)\n",
    "Test= Test.drop(['is.test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train['vector_gensim_product'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "query_item_order_test=[]\n",
    "labels_test=[]\n",
    "for index,row in Test.iterrows():\n",
    "    r=[]\n",
    "    query_item_order_test.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['cosine'])\n",
    "    r.append(row['graphfeature0'])\n",
    "    r.append(row['graphfeature1'])\n",
    "    r.append(row['graphfeature2'])\n",
    "    r.append(row['graphfeature3'])\n",
    "    r.append(row['graphfeature4'])\n",
    "    r.append(row['graphfeature5'])\n",
    "    r.append(row['graphfeature6'])\n",
    "    r.append(row['graphfeature7'])\n",
    "    r.append(row['graphfeature8'])\n",
    "    r.append(row['graphfeature9'])\n",
    "    r.append(row['graphfeature10'])\n",
    "    r.append(row['graphfeature11'])\n",
    "    r.append(row['graphfeature12'])\n",
    "    r.append(row['graphfeature13'])\n",
    "    r.append(row['graphfeature14'])\n",
    "    r.append(row['graphfeature15'])\n",
    "    d1= row['vector_gensim_query'].tolist()\n",
    "    r= r + d1[:49]\n",
    "    d2= row['vector_gensim_product'].tolist()\n",
    "    r= r + d2[:49]\n",
    "    \n",
    "    test.append(r)\n",
    "    labels_test.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "query_item_order=[]\n",
    "labels=[]\n",
    "for index,row in Train.iterrows():\n",
    "    r=[]\n",
    "    query_item_order.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['cosine'])\n",
    "    r.append(row['graphfeature0'])\n",
    "    r.append(row['graphfeature1'])\n",
    "    r.append(row['graphfeature2'])\n",
    "    r.append(row['graphfeature3'])\n",
    "    r.append(row['graphfeature4'])\n",
    "    r.append(row['graphfeature5'])\n",
    "    r.append(row['graphfeature6'])\n",
    "    r.append(row['graphfeature7'])\n",
    "    r.append(row['graphfeature8'])\n",
    "    r.append(row['graphfeature9'])\n",
    "    r.append(row['graphfeature10'])\n",
    "    r.append(row['graphfeature11'])\n",
    "    r.append(row['graphfeature12'])\n",
    "    r.append(row['graphfeature13'])\n",
    "    r.append(row['graphfeature14'])\n",
    "    r.append(row['graphfeature15'])\n",
    "\n",
    "    d1= row['vector_gensim_query'].tolist()\n",
    "    r= r + d1[:49]\n",
    "    d2= row['vector_gensim_product'].tolist()\n",
    "    r= r + d2[:49]\n",
    "    \n",
    "    train.append(r)\n",
    "    labels.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test= np.array(test)\n",
    "sc = StandardScaler()\n",
    "train = sc.fit_transform(train)\n",
    "test = sc.transform(test)\n",
    "L=Train['queryId'].value_counts(sort=True, ascending=True)\n",
    "L = dict(L)\n",
    "L= OrderedDict(sorted(L.items()))\n",
    "print(len(L))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/\n",
    "k = 0\n",
    "Xp, yp, diff = [], [], []\n",
    "s=0\n",
    "e=0\n",
    "theta = np.deg2rad(60)\n",
    "w = np.array([np.sin(theta), np.cos(theta)]) #for plotting\n",
    "for key in L:\n",
    "    comb = itertools.combinations(range(L.get(key)), 2)\n",
    "    #create the first batch\n",
    "    s=e\n",
    "    e=s+L.get(key) -1\n",
    "    #print(\"Start:%d, End %d\" %(s,e))\n",
    "    for (i, j) in comb:\n",
    "        i=i+s\n",
    "        j=j+e-1\n",
    "        if labels[i] == labels[j] or query_item_order[i][0] != query_item_order[j][0] :\n",
    "            # skip if same target or different group(different qIds)\n",
    "            continue\n",
    "        Xp.append(train[i] - train[j])\n",
    "        diff.append(labels[i] - labels[j])\n",
    "        yp.append(np.sign(diff[-1]))\n",
    "        # output balanced classes\n",
    "        if yp[-1] != (-1) ** k:\n",
    "            yp[-1] *= -1\n",
    "            Xp[-1] *= -1\n",
    "            diff[-1] *= -1\n",
    "        k += 1\n",
    "Xp, yp, diff = map(np.asanyarray, (Xp, yp, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=.1)\n",
    "clf.fit(Xp, yp)\n",
    "coef = clf.coef_.ravel() / linalg.norm(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the prediction \n",
    "pred= np.dot(test, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process before Writing to a file\n",
    "print(pred.shape)\n",
    "print(len(pred))\n",
    "od=Test['queryId'].value_counts(sort=True, ascending=True)\n",
    "od = dict(od)\n",
    "od= OrderedDict(sorted(od.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Scores and Get the order\n",
    "\n",
    "q_i={}\n",
    "\n",
    "s=0\n",
    "e=0\n",
    "for key in od:\n",
    "    s=e\n",
    "    e=s+od.get(key)\n",
    "    print(\"Start:%d, End %d\" %(s,e))\n",
    "    subset= query_item_order_test[s:e]\n",
    "    query_item_order=[]\n",
    "    query_item_score=[]\n",
    "    for item in subset:\n",
    "        query_item_order.append(item[1])\n",
    "        query_item_score.append(pred[s])\n",
    "        s=s+1\n",
    "    temp=np.argsort(query_item_score)[::-1]\n",
    "    temp=temp.tolist()\n",
    "    query_item_order= np.array(query_item_order)\n",
    "    query_item_order = query_item_order[temp]\n",
    "    q_i[key]= query_item_order.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a file \n",
    "f= open(\"D:\\\\Courses\\\\InfoStorage\\\\Project\\\\Data\\\\submissions\\\\RankSVMGraphFeatures100WithEmbeddingsallCosineBasic.txt\",\"w\")\n",
    "for key in q_i:\n",
    "    f.write(\"%s \" % key)\n",
    "    for index,item in enumerate(q_i.get(key)):\n",
    "        ll=len(q_i.get(key))-1\n",
    "        item= int(item)\n",
    "        if index < ll:\n",
    "            f.write(\"%s,\" % item)\n",
    "        else:\n",
    "            f.write(\"%s\" % item)\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qio = np.array(query_item_order)\n",
    "lbls = np.array(labels)\n",
    "qio_test = np.array(query_item_order_test)\n",
    "lbls_test = np.array(labels_test)\n",
    "# np.loadtxt np.savetxt\n",
    "path_to_save=\"D:\\\\Courses\\\\InfoStorage\\\\Project\\\\Data\\\\Numpy_Objects\\\\\"\n",
    "X.to_pickle(path_to_save+\"AllFeaturesCombined.pkl\")\n",
    "Train.to_pickle(path_to_save+\"Train.pkl\")\n",
    "Test.to_pickle(path_to_save+\"Test.pkl\")\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddings8_train.txt',train,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddingsAfterPairWise8_Xp.txt',Xp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddingsAfterPairWise8_yp.txt',yp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddings8_test.txt',test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddings8_query_item_order.txt',qio,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddings8_labels.txt',lbls,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddings8_query_item_order_test.txt',qio_test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'GraphFeatures100WithWord2VecEmbeddings8_labels_test.txt',lbls_test,fmt='%1.15f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Features + All Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = X[X['is.test']==False]\n",
    "Test = X[X['is.test']== True]\n",
    "Train= Train.drop(['is.test'], axis=1)\n",
    "Test= Test.drop(['is.test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "query_item_order_test=[]\n",
    "labels_test=[]\n",
    "for index,row in Test.iterrows():\n",
    "    r=[]\n",
    "    query_item_order_test.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['item_show_count'])\n",
    "    r.append(row['Item_clicks'])\n",
    "    r.append(row['Item_view_count'])\n",
    "    r.append(row['Item_purchase_count'])\n",
    "    r.append(row['CTR'])\n",
    "    r.append(row['userShow'])\n",
    "    r.append(row['userView'])\n",
    "    r.append(row['userPurchase'])\n",
    "    r.append(row['View Rate'])\n",
    "    r.append(row['Click Value Rate'])\n",
    "    r= r + row['vector_gensim_query'].tolist()\n",
    "    r= r + row['vector_gensim_product'].tolist()\n",
    "    \n",
    "    test.append(r)\n",
    "    labels_test.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "query_item_order=[]\n",
    "labels=[]\n",
    "for index,row in Train.iterrows():\n",
    "    r=[]\n",
    "    query_item_order.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['item_show_count'])\n",
    "    r.append(row['Item_clicks'])\n",
    "    r.append(row['Item_view_count'])\n",
    "    r.append(row['Item_purchase_count'])\n",
    "    r.append(row['CTR'])\n",
    "    r.append(row['userShow'])\n",
    "    r.append(row['userView'])\n",
    "    r.append(row['userPurchase'])\n",
    "    r.append(row['View Rate'])\n",
    "    r.append(row['Click Value Rate'])\n",
    "    r= r + row['vector_gensim_query'].tolist()\n",
    "    r= r + row['vector_gensim_product'].tolist()\n",
    "    \n",
    "    train.append(r)\n",
    "    labels.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test= np.array(test)\n",
    "sc = StandardScaler()\n",
    "train = sc.fit_transform(train)\n",
    "test = sc.transform(test)\n",
    "L=Train['queryId'].value_counts(sort=True, ascending=True)\n",
    "L = dict(L)\n",
    "L= OrderedDict(sorted(L.items()))\n",
    "print(len(L))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/\n",
    "k = 0\n",
    "Xp, yp, diff = [], [], []\n",
    "s=0\n",
    "e=0\n",
    "theta = np.deg2rad(60)\n",
    "w = np.array([np.sin(theta), np.cos(theta)]) #for plotting\n",
    "for key in L:\n",
    "    comb = itertools.combinations(range(L.get(key)), 2)\n",
    "    #create the first batch\n",
    "    s=e\n",
    "    e=s+L.get(key) -1\n",
    "    #print(\"Start:%d, End %d\" %(s,e))\n",
    "    for (i, j) in comb:\n",
    "        i=i+s\n",
    "        j=j+e-1\n",
    "        if labels[i] == labels[j] or query_item_order[i][0] != query_item_order[j][0] :\n",
    "            # skip if same target or different group(different qIds)\n",
    "            continue\n",
    "        Xp.append(train[i] - train[j])\n",
    "        diff.append(labels[i] - labels[j])\n",
    "        yp.append(np.sign(diff[-1]))\n",
    "        # output balanced classes\n",
    "        if yp[-1] != (-1) ** k:\n",
    "            yp[-1] *= -1\n",
    "            Xp[-1] *= -1\n",
    "            diff[-1] *= -1\n",
    "        k += 1\n",
    "Xp, yp, diff = map(np.asanyarray, (Xp, yp, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qio = np.array(query_item_order)\n",
    "lbls = np.array(labels)\n",
    "qio_test = np.array(query_item_order_test)\n",
    "lbls_test = np.array(labels_test)\n",
    "# np.loadtxt np.savetxt\n",
    "path_to_save=\"D:\\\\Courses\\\\InfoStorage\\\\Project\\\\Data\\\\Numpy_Objects\\\\\"\n",
    "X.to_pickle(path_to_save+\"AllFeaturesCombined.pkl\")\n",
    "Train.to_pickle(path_to_save+\"Train.pkl\")\n",
    "Test.to_pickle(path_to_save+\"Test.pkl\")\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsall_train.txt',train,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsAfterPairWiseall_Xp.txt',Xp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsAfterPairWiseall_yp.txt',yp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsall_test.txt',test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsall_query_item_order.txt',qio,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsall_labels.txt',lbls,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsall_query_item_order_test.txt',qio_test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddingsall_labels_test.txt',lbls_test,fmt='%1.15f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Features + Reduced Embeddings 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_gensimProduct8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_gensimQuery8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['vector_gensim_query','vector_gensim_product'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X,vector_gensimProduct6,on='itemId',how='left')\n",
    "X = pd.merge(X,vector_gensimQuery6,on='queryId',how='left')\n",
    "X.head()\n",
    "Train = X[X['is.test']==False]\n",
    "Test = X[X['is.test']== True]\n",
    "Train= Train.drop(['is.test'], axis=1)\n",
    "Test= Test.drop(['is.test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "query_item_order_test=[]\n",
    "labels_test=[]\n",
    "for index,row in Test.iterrows():\n",
    "    r=[]\n",
    "    query_item_order_test.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['item_show_count'])\n",
    "    r.append(row['Item_clicks'])\n",
    "    r.append(row['Item_view_count'])\n",
    "    r.append(row['Item_purchase_count'])\n",
    "    r.append(row['CTR'])\n",
    "    r.append(row['userShow'])\n",
    "    r.append(row['userView'])\n",
    "    r.append(row['userPurchase'])\n",
    "    r.append(row['View Rate'])\n",
    "    r.append(row['Click Value Rate'])\n",
    "    r= r + row['vector_gensim_query'].tolist()\n",
    "    r= r + row['vector_gensim_product'].tolist()\n",
    "    \n",
    "    test.append(r)\n",
    "    labels_test.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "query_item_order=[]\n",
    "labels=[]\n",
    "for index,row in Train.iterrows():\n",
    "    r=[]\n",
    "    query_item_order.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['item_show_count'])\n",
    "    r.append(row['Item_clicks'])\n",
    "    r.append(row['Item_view_count'])\n",
    "    r.append(row['Item_purchase_count'])\n",
    "    r.append(row['CTR'])\n",
    "    r.append(row['userShow'])\n",
    "    r.append(row['userView'])\n",
    "    r.append(row['userPurchase'])\n",
    "    r.append(row['View Rate'])\n",
    "    r.append(row['Click Value Rate'])\n",
    "    r= r + row['vector_gensim_query'].tolist()\n",
    "    r= r + row['vector_gensim_product'].tolist()\n",
    "    \n",
    "    train.append(r)\n",
    "    labels.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test= np.array(test)\n",
    "sc = StandardScaler()\n",
    "train = sc.fit_transform(train)\n",
    "test = sc.transform(test)\n",
    "L=Train['queryId'].value_counts(sort=True, ascending=True)\n",
    "L = dict(L)\n",
    "L= OrderedDict(sorted(L.items()))\n",
    "print(len(L))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/\n",
    "k = 0\n",
    "Xp, yp, diff = [], [], []\n",
    "s=0\n",
    "e=0\n",
    "theta = np.deg2rad(60)\n",
    "w = np.array([np.sin(theta), np.cos(theta)]) #for plotting\n",
    "for key in L:\n",
    "    comb = itertools.combinations(range(L.get(key)), 2)\n",
    "    #create the first batch\n",
    "    s=e\n",
    "    e=s+L.get(key) -1\n",
    "    print(\"Start:%d, End %d\" %(s,e))\n",
    "    for (i, j) in comb:\n",
    "        i=i+s\n",
    "        j=j+e-1\n",
    "        if labels[i] == labels[j] or query_item_order[i][0] != query_item_order[j][0] :\n",
    "            # skip if same target or different group(different qIds)\n",
    "            continue\n",
    "        Xp.append(train[i] - train[j])\n",
    "        diff.append(labels[i] - labels[j])\n",
    "        yp.append(np.sign(diff[-1]))\n",
    "        # output balanced classes\n",
    "        if yp[-1] != (-1) ** k:\n",
    "            yp[-1] *= -1\n",
    "            Xp[-1] *= -1\n",
    "            diff[-1] *= -1\n",
    "        k += 1\n",
    "Xp, yp, diff = map(np.asanyarray, (Xp, yp, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qio = np.array(query_item_order)\n",
    "lbls = np.array(labels)\n",
    "qio_test = np.array(query_item_order_test)\n",
    "lbls_test = np.array(labels_test)\n",
    "# np.loadtxt np.savetxt\n",
    "path_to_save=\"D:\\\\Courses\\\\InfoStorage\\\\Project\\\\Data\\\\Numpy_Objects\\\\\"\n",
    "X.to_pickle(path_to_save+\"AllFeaturesCombined.pkl\")\n",
    "Train.to_pickle(path_to_save+\"Train.pkl\")\n",
    "Test.to_pickle(path_to_save+\"Test.pkl\")\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8_train.txt',train,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8AfterPairWise_Xp.txt',Xp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8AfterPairWise_yp.txt',yp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8_test.txt',test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8_query_item_order.txt',qio,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8_labels.txt',lbls,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8_query_item_order_test.txt',qio_test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings8_labels_test.txt',lbls_test,fmt='%1.15f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Features + Reduced Embeddings 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_gensimProduct6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_gensimQuery6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['vector_gensim_query','vector_gensim_product'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X,vector_gensimProduct6,on='itemId',how='left')\n",
    "X = pd.merge(X,vector_gensimQuery6,on='queryId',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = X[X['is.test']==False]\n",
    "Test = X[X['is.test']== True]\n",
    "Train= Train.drop(['is.test'], axis=1)\n",
    "Test= Test.drop(['is.test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "query_item_order_test=[]\n",
    "labels_test=[]\n",
    "for index,row in Test.iterrows():\n",
    "    r=[]\n",
    "    query_item_order_test.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['item_show_count'])\n",
    "    r.append(row['Item_clicks'])\n",
    "    r.append(row['Item_view_count'])\n",
    "    r.append(row['Item_purchase_count'])\n",
    "    r.append(row['CTR'])\n",
    "    r.append(row['userShow'])\n",
    "    r.append(row['userView'])\n",
    "    r.append(row['userPurchase'])\n",
    "    r.append(row['View Rate'])\n",
    "    r.append(row['Click Value Rate'])\n",
    "    r= r + row['vector_gensim_query'].tolist()\n",
    "    r= r + row['vector_gensim_product'].tolist()\n",
    "    \n",
    "    test.append(r)\n",
    "    labels_test.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "query_item_order=[]\n",
    "labels=[]\n",
    "for index,row in Train.iterrows():\n",
    "    r=[]\n",
    "    query_item_order.append([row['queryId'],row['itemId']])\n",
    "    \n",
    "    r.append(row['item_show_count'])\n",
    "    r.append(row['Item_clicks'])\n",
    "    r.append(row['Item_view_count'])\n",
    "    r.append(row['Item_purchase_count'])\n",
    "    r.append(row['CTR'])\n",
    "    r.append(row['userShow'])\n",
    "    r.append(row['userView'])\n",
    "    r.append(row['userPurchase'])\n",
    "    r.append(row['View Rate'])\n",
    "    r.append(row['Click Value Rate'])\n",
    "    r= r + row['vector_gensim_query'].tolist()\n",
    "    r= r + row['vector_gensim_product'].tolist()\n",
    "    \n",
    "    train.append(r)\n",
    "    labels.append(row['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test= np.array(test)\n",
    "sc = StandardScaler()\n",
    "train = sc.fit_transform(train)\n",
    "test = sc.transform(test)\n",
    "L=Train['queryId'].value_counts(sort=True, ascending=True)\n",
    "L = dict(L)\n",
    "L= OrderedDict(sorted(L.items()))\n",
    "print(len(L))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/\n",
    "k = 0\n",
    "Xp, yp, diff = [], [], []\n",
    "s=0\n",
    "e=0\n",
    "theta = np.deg2rad(60)\n",
    "w = np.array([np.sin(theta), np.cos(theta)]) #for plotting\n",
    "for key in L:\n",
    "    comb = itertools.combinations(range(L.get(key)), 2)\n",
    "    #create the first batch\n",
    "    s=e\n",
    "    e=s+L.get(key) -1\n",
    "    print(\"Start:%d, End %d\" %(s,e))\n",
    "    for (i, j) in comb:\n",
    "        i=i+s\n",
    "        j=j+e-1\n",
    "        if labels[i] == labels[j] or query_item_order[i][0] != query_item_order[j][0] :\n",
    "            # skip if same target or different group(different qIds)\n",
    "            continue\n",
    "        Xp.append(train[i] - train[j])\n",
    "        diff.append(labels[i] - labels[j])\n",
    "        yp.append(np.sign(diff[-1]))\n",
    "        # output balanced classes\n",
    "        if yp[-1] != (-1) ** k:\n",
    "            yp[-1] *= -1\n",
    "            Xp[-1] *= -1\n",
    "            diff[-1] *= -1\n",
    "        k += 1\n",
    "Xp, yp, diff = map(np.asanyarray, (Xp, yp, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qio = np.array(query_item_order)\n",
    "lbls = np.array(labels)\n",
    "qio_test = np.array(query_item_order_test)\n",
    "lbls_test = np.array(labels_test)\n",
    "# np.loadtxt np.savetxt\n",
    "path_to_save=\"D:\\\\Courses\\\\InfoStorage\\\\Project\\\\Data\\\\Numpy_Objects\\\\\"\n",
    "X.to_pickle(path_to_save+\"AllFeaturesCombined.pkl\")\n",
    "Train.to_pickle(path_to_save+\"Train.pkl\")\n",
    "Test.to_pickle(path_to_save+\"Test.pkl\")\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6_train.txt',train,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6AfterPairWise_Xp.txt',Xp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6AfterPairWise_yp.txt',yp,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6_test.txt',test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6_query_item_order.txt',qio,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6_labels.txt',lbls,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6_query_item_order_test.txt',qio_test,fmt='%1.15f')\n",
    "np.savetxt(path_to_save+'allFeaturesWithWord2VecEmbeddings6_labels_test.txt',lbls_test,fmt='%1.15f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
