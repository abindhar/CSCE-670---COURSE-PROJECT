{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "queries = pd.read_csv('DATA/train-queries.csv', sep=';')\n",
    "item_views = pd.read_csv('DATA/train-item-views.csv', sep=';')\n",
    "clicks = pd.read_csv('DATA/train-clicks.csv', sep=';')\n",
    "purchases = pd.read_csv('DATA/train-purchases.csv', sep=';')\n",
    "products = pd.read_csv('DATA/products.csv', sep=';')\n",
    "products_category = pd.read_csv('DATA/product-categories.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics OF DATASET :\n",
    "\n",
    "query_full_queries_count  = len(queries[~queries[\"searchstring.tokens\"].isnull()])\n",
    "query_less_queries_count  = len(queries[queries[\"searchstring.tokens\"].isnull()])\n",
    "sessions_count = len(queries[\"sessionId\"].unique())\n",
    "\n",
    "#presented products 130,987  : not sure.\n",
    "\n",
    "clicks_log_count = len(clicks)\n",
    "views_log_count = len(item_views)\n",
    "purchase_log_count  = len(purchases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Statistics of Dataset\n",
    "\n",
    "real_users_count = len(queries[~queries[\"userId\"].isnull()][\"userId\"].unique())\n",
    "anonymous_users_count = len(queries[queries[\"userId\"].isnull()])\n",
    "train_real_users_count = len(queries[queries[\"is.test\"] == False][~queries[\"userId\"].isnull()][\"userId\"].unique())\n",
    "test_real_users_count = len(queries[queries[\"is.test\"] == True][~queries[\"userId\"].isnull()][\"userId\"].unique())\n",
    "\n",
    "#intersection between test and trAIN USERS \n",
    "\n",
    "unique_train_users = queries[queries[\"is.test\"] == False][~queries[\"userId\"].isnull()][\"userId\"].unique()\n",
    "uniquer_test_users = queries[queries[\"is.test\"] == True][~queries[\"userId\"].isnull()][\"userId\"].unique()\n",
    "train_inter_test_usercount = len(np.intersect1d(unique_train_users,uniquer_test_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_full_queries_count :  51888\n",
      "query_less_queries_count :  871239\n",
      "sessions_count :  573935\n",
      "clicks_log_count :  1127764\n",
      "views_log_count :  1235380\n",
      "purchase_log_count :  18025\n",
      "\n",
      "\n",
      "real_users_count :  232816\n",
      "anonymous_users_count :  574887\n",
      "train_real_users_count :  140387\n",
      "test_real_users_count :  116630\n",
      "train_inter_test_usercount :  24201\n"
     ]
    }
   ],
   "source": [
    "#print statistics\n",
    "\n",
    "\n",
    "print(\"query_full_queries_count : \",query_full_queries_count)\n",
    "print(\"query_less_queries_count : \",query_less_queries_count)\n",
    "print(\"sessions_count : \",sessions_count)\n",
    "print(\"clicks_log_count : \",clicks_log_count)\n",
    "print(\"views_log_count : \",views_log_count)\n",
    "print(\"purchase_log_count : \",purchase_log_count)\n",
    "\n",
    "#print user statistics\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"real_users_count : \",real_users_count)\n",
    "print(\"anonymous_users_count : \",anonymous_users_count)\n",
    "print(\"train_real_users_count : \",train_real_users_count)\n",
    "print(\"test_real_users_count : \",test_real_users_count)\n",
    "print(\"train_inter_test_usercount : \",train_inter_test_usercount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRE-PROCESSING FOR FEATURE EXTRACTION\n",
    "\n",
    "queries = queries[~queries[\"searchstring.tokens\"].isnull()]\n",
    "\n",
    "# Extract a mapping of each query and which items appeared\n",
    "query_item = []\n",
    "for query, items in queries[[\"queryId\", \"items\"]].values:\n",
    "    items = map(np.int64,items.split(','))\n",
    "    for i in items:\n",
    "        query_item.append( (query, i) )\n",
    "query_item = pd.DataFrame().from_records(query_item, columns=[\"queryId\",\"itemId\"])\n",
    "\n",
    "item_views.sort_values([\"sessionId\", \"userId\", \"eventdate\", \"timeframe\", \"itemId\"], inplace=True)\n",
    "clicks.sort_values([\"queryId\", \"timeframe\", \"itemId\"], inplace=True)\n",
    "purchases.sort_values([\"sessionId\", \"userId\", \"eventdate\", \"timeframe\", \"itemId\", \"ordernumber\"], inplace=True)\n",
    "products.sort_values([\"itemId\"], inplace=True)\n",
    "products_category.sort_values([\"itemId\"], inplace=True)\n",
    "\n",
    "\n",
    "query_item = pd.merge(query_item, queries[[\"queryId\", \"sessionId\"]], how=\"left\")\n",
    "query_item = pd.merge(query_item,  clicks, how=\"left\")\n",
    "query_item.rename(columns={\"timeframe\":\"clickTime\"}, inplace=True)\n",
    "query_item = pd.merge(query_item,  item_views, how=\"left\")\n",
    "\n",
    "query_item.rename(columns={\"eventdate\":\"eventdateView\", \"timeframe\":\"viewTime\", \"userId\": \"userView\"}, inplace=True)\n",
    "query_item = pd.merge(query_item, purchases, how=\"left\")\n",
    "query_item.rename(columns={\"eventdate\":\"eventdatePurchase\", \"timeframe\":\"purchaseTime\", \"userId\": \"userPurchase\"}, inplace=True)\n",
    "\n",
    "\n",
    "query_item[\"clicked\"] = ~query_item[\"clickTime\"].isnull()\n",
    "query_item[\"viewed\"] = ~query_item[\"viewTime\"].isnull()\n",
    "query_item[\"purchased\"] = ~query_item[\"purchaseTime\"].isnull()\n",
    "\n",
    "\n",
    "user_item = []\n",
    "for user, items in queries[[\"userId\", \"items\"]].values:\n",
    "    items = map(np.int64,items.split(','))\n",
    "    for i in items:\n",
    "        user_item.append( (user, i) )\n",
    "user_item = pd.DataFrame().from_records(user_item, columns=[\"userShow\",\"itemId\"])\n",
    "user_item = user_item[~user_item[\"userShow\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "#Extracting global statistical features for each product\n",
    "\n",
    "item_clicks = clicks.groupby(\"itemId\").count().rename(columns = {\"queryId\" :\"Item_clicks\"})\n",
    "product_views = item_views.groupby(\"itemId\").count().rename(columns = {\"sessionId\" : \"Item_view_count\"})\n",
    "item_purchases = purchases.groupby(\"itemId\").count().rename(columns = {\"sessionId\" : \"Item_purchase_count\"})\n",
    "item_show = query_item.groupby(\"itemId\").count().rename(columns = {\"queryId\" : \"item_show_count\"})\n",
    "\n",
    "userShow = user_item[[\"itemId\",\"userShow\"]].groupby(\"itemId\")[\"userShow\"].unique().apply(lambda x:len(x)).to_frame()\n",
    "userView = pd.DataFrame(query_item[[\"itemId\",\"userView\"]].groupby(\"itemId\")[\"userView\"].unique().apply(lambda x:len(x)))\n",
    "userPurchase = pd.DataFrame(query_item[[\"itemId\",\"userPurchase\"]].groupby(\"itemId\")[\"userPurchase\"].unique().apply(lambda x:len(x)))\n",
    "\n",
    "dfs = [products[[\"itemId\"]],item_show[[\"item_show_count\"]],item_clicks[[\"Item_clicks\"]],product_views[[\"Item_view_count\"]],item_purchases[[\"Item_purchase_count\"]],userShow,userView,userPurchase]\n",
    "product_stats = reduce(lambda left,right: pd.merge(left,right,on='itemId',how=\"left\"), dfs).sort_values([\"itemId\"]).reset_index().drop(\"index\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_stats[\"CTR\"] = product_stats[\"Item_clicks\"]/product_stats[\"item_show_count\"]\n",
    "product_stats[\"View Rate\"] = product_stats[\"Item_view_count\"]/product_stats[\"item_show_count\"]\n",
    "product_stats[\"Click Value Rate\"] = product_stats[\"Item_purchase_count\"]/product_stats[\"item_show_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countwords(row):\n",
    "    return row.count(\",\")+1\n",
    "\n",
    "product_stats[\"wordLength\"] = products[\"product.name.tokens\"].apply(countwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_info = pd.merge(query_item[[\"queryId\", \"itemId\"]].drop_duplicates(), queries[[\"queryId\", \"searchstring.tokens\"]], on=\"queryId\", how=\"left\").merge(products, on=\"itemId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Embedding models for queries and product tokens in their space by mapping each as a document. If results not significant to use word2vec\n",
    "#To later save these models and load them rather than generating the model everytime\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def func2(df_t):\n",
    "    return df_t.split(',')\n",
    "\n",
    "reviews_gensim = list(queries[\"searchstring.tokens\"].apply(func2))\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews_gensim)]\n",
    "model = Doc2Vec(documents, vector_size=50, min_count=1, workers=4,epochs=100,verbose = True) \n",
    "\n",
    "\n",
    "def findVector(df_val): #finding vector for the reviews based on doc2vec model\n",
    "    review_words = df_val.split()\n",
    "    return model.infer_vector(review_words)\n",
    "\n",
    "queries['vector_gensim_query'] = queries[\"searchstring.tokens\"].apply(findVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(df_t):\n",
    "    return df_t.split(',')\n",
    "\n",
    "reviews_gensim1 = list(products[\"product.name.tokens\"].apply(func2))\n",
    "\n",
    "documents1 = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews_gensim1)]\n",
    "model2 = Doc2Vec(documents1, vector_size=50, min_count=1, workers=4,epochs=100,verbose = True) \n",
    "\n",
    "\n",
    "def findVector1(df_val): #finding vector for the reviews based on doc2vec model\n",
    "    review_words = df_val.split()\n",
    "    return model2.infer_vector(review_words)\n",
    "\n",
    "products['vector_gensim_product'] = products[\"product.name.tokens\"].apply(findVector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_item =  pd.merge(query_item,queries[[\"queryId\",\"vector_gensim_query\"]],on=\"queryId\",how=\"left\")\n",
    "product_stats = pd.merge(product_stats,products[[\"itemId\",\"vector_gensim_product\"]],on=\"itemId\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining relevance label\n",
    "\n",
    "def relevance(row):\n",
    "    if row[\"purchased\"] == True:\n",
    "        return 2\n",
    "    elif row[\"clicked\"] == True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "query_item[\"Relevance\"] = query_item.apply(relevance,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_item_input = query_item[[\"queryId\",\"itemId\",\"vector_gensim_query\",\"Relevance\"]]\n",
    "query_item_input = pd.merge(query_item_input,queries[[\"queryId\",\"is.test\"]],on=\"queryId\",how=\"left\")\n",
    "queries_input = queries[[\"queryId\",\"items\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_stats.to_pickle(\"product_stats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_item_input.to_pickle(\"query_item.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_input.to_pickle(\"queries_input.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processed data stored in three pickle objects. product_stats.pkl, query_item.pkl, queries_input.pkl\n",
    "\n",
    "###SHARED THE PICKLE FILE IN DRIVE AS DATA IS HUGE TO UPLOAD IN GITHUB. PATH : \"info_project_files/CSCE-670---COURSE-PROJECT/Feature Extraction: pickle objects \" ###\n",
    "\n",
    "#queries_input.pkl : shows the original queryID and corresponding items order. Output should be in this format. Also refer to \"baseline_submission.txt\" for further format.\n",
    "#query_item.pkl: contains the queryID, itemID corresponding to each input to model. It contains FEATURE 1: QUERY EMBEDDING VECTOR. AND OUTPUT LABELS: Relevance scores for each query item pair.\n",
    "#product_stats.pkl : contains the product features. \n",
    "    \n",
    "    \n",
    "# STEPS:\n",
    "# 1) Read the pickles. Format : df = pd.read_pickle(\"product_stats.pkl\")\n",
    "# 2) Separate train, test from query_item.pkl and the output labels.\n",
    "# 3) Take a data point from train. \n",
    "#   Eg: (a) 1st train point is queryID : 1, ItemID : 7518 .\n",
    "#       (b) FEATURE 1: Query Embedding vector : vector_gensim_query\n",
    "#       (c) For itemID : 7518, get its Product Embedding vector from product_stats.pkl dataframe ie, \n",
    "#           product_stats[product_stats[\"itemId\"]== 7518][\"vector_gensim_product\"]\n",
    "#       (d) Get all other product features except vector_gensim_product of item id = 7518, ie,\n",
    "#          product_stats[product_stats[\"itemId\"]== 7518] except vector_gensim_product\n",
    "#       (e) FEATURE 1: WORD EMBEDDING FOR QUERY, FEATURE 2: WORD EMBEDDING FOR PRODUCT, FEATURE 3: PRODUCT FEATURES, LABEL : RELEVANCE\n",
    "#       (f) Train the features this way and test the test data and compare the relavance using the prediction model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional features yet to do: \n",
    "\n",
    "# 1) Session based features\n",
    "# 2) Time based Global statistical features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
